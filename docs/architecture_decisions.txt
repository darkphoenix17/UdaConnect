UdaConnect application consists of 3 parts: frontend: udaconnect-app, backend: udaconnect-api, database: postgres. 

I decided to refactor the backend "udaconnect-api" monolith application by refactoring it into 3 entity microservices: Persons, Connections, Locations.
Entity 1) "Persons"-> It could be separated into its own microservice container "persons-api", it has the only dependancy on the database. It exposes 2 API endpoints:
http://localhost:30001/api/persons
http://localhost:30001/api/persons/<person_id>
The complexity of refactoring: lowest, Hence, the most simple part to refactor.

Entity 2) "Connections"-> It could be separated into its own microservice container. Initially it code was set to fetch persons and locations data from the database by making the map of where the specified by frontend app person was located, making that requests by the response to frontend app to its API endpoint:
http://localhost:30002/api/persons/<person_id>/connection
Both requests to fetch persons and locations data were used to send to the postgres database. Rearranged the request dedicated to persons by routing it to the "http://localhost:30001/api/persons" of "persons-api" microservice. So, I also thought about the similar approach regarding the locations data, the location data is fetched directly from the database.
The complexity of refactoring: average.

Entity 3) "Locations"-> As per Instructions, design should be able to handle a large volume of data, by the way, initially it also had an endpoint:
http://localhost:30001/api/locations
http://localhost:30001/api/locations/<location_id>
-, the first of which accepted Post method for creating new location, and the second one was for retrieving the specified location data (only one location by its id, by initial design), but I decided to omit that endpoints due to the fact it wasn't used in frontend app and my decision not to use that as the endpoint for "Connections" entity microservice, as I did with "persons-api". My design is to separate that service to 3 microservices: 
(a) "locations-generator": these containers will get raw data from various sensors, normalize that data and transfer to the second microservice type (see next);
(b) "locations2kafka": huge of burstly risen amount of data could make your database software suffer from the load, and where it is the Kafka event streaming platform comes to help us - "locations-kafka" containers put the messages into the Kafka "locations" topic (and it's assumed the overall storage and performance of Kafka cluster will suffice, as it has high horizontal extensibility, which our fictional project laboratory can afford).
And, by the way, the communication channel between "location-generator" and "locations-kafka" was chosen to be the gRPC protocol, it has multiple times more performance gain in comparison with traditional RESTapi communication, especially here it would be so useful, where the performance gain is needed.
(—Å) "locations-kafka2db": meanwhile the above mentioned work is being performed, this type of service containers will fetch with monotonous performance the location data messages from the Kafka topic, parse them and put in DB's "location" table.